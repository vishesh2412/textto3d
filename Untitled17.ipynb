{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Text to 3D Point Cloud Generator\n",
        "\n",
        "This script generates detailed 3D point clouds from text descriptions using\n",
        "a combination of Stable Diffusion for image generation and MiDaS for depth estimation.\n",
        "The output is a complete, textured point cloud in PLY format.\n",
        "\n",
        "Requirements:\n",
        "- torch\n",
        "- numpy\n",
        "- matplotlib\n",
        "- open3d\n",
        "- timm\n",
        "- PIL\n",
        "- cv2\n",
        "\"\"\"\n",
        "\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q open3d timm\n",
        "!pip install -q opencv-python matplotlib\n",
        "!pip install -q git+https://github.com/CompVis/stable-diffusion.git@main\n",
        "!pip install -q pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/$(python -c \"import torch; print(f'torch-{torch.__version__}_cu' + torch.version.cuda.replace('.','') + '_pyt' + torch.__version__.split('+')[0])\").wheels.html\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import open3d as o3d\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import time\n",
        "import requests\n",
        "import urllib.request\n",
        "from io import BytesIO\n",
        "import zipfile\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Install required packages\n",
        "\n",
        "# Download MiDaS\n",
        "print(\"Downloading MiDaS model...\")\n",
        "midas_path = \"MiDaS\"\n",
        "if not os.path.exists(midas_path):\n",
        "    os.makedirs(midas_path)\n",
        "    # Download MiDaS code\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://github.com/isl-org/MiDaS/archive/refs/heads/master.zip\",\n",
        "        \"midas.zip\"\n",
        "    )\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(\"midas.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\".\")\n",
        "\n",
        "    # Rename the folder\n",
        "    os.rename(\"MiDaS-master\", midas_path)\n",
        "\n",
        "    # Download MiDaS model weights\n",
        "    os.makedirs(os.path.join(midas_path, \"weights\"), exist_ok=True)\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://github.com/isl-org/MiDaS/releases/download/v3_1/dpt_beit_large_512.pt\",\n",
        "        os.path.join(midas_path, \"weights\", \"dpt_beit_large_512.pt\")\n",
        "    )\n",
        "\n",
        "# Add MiDaS to path\n",
        "import sys\n",
        "sys.path.append(midas_path)\n",
        "\n",
        "# Import MiDaS modules\n",
        "from MiDaS.midas.model_loader import load_model\n",
        "from MiDaS.midas.transforms import Resize, NormalizeImage, PrepareForNet\n",
        "from MiDaS.midas.dpt_depth import DPTDepthModel\n",
        "\n",
        "# Import Stable Diffusion\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "from ldm.util import instantiate_from_config\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "class TextTo3DPointCloud:\n",
        "    def __init__(self, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        self.device = device\n",
        "        self.resolution = 512\n",
        "        self.num_inference_steps = 50\n",
        "        self.strength = 0.75\n",
        "        self.angle_views = 8  # Number of views to generate\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_models(self):\n",
        "        \"\"\"Set up the necessary models for text-to-image and depth estimation.\"\"\"\n",
        "        print(\"Setting up models...\")\n",
        "\n",
        "        # Download and load the Stable Diffusion model\n",
        "        print(\"Setting up Stable Diffusion model...\")\n",
        "        sd_config = OmegaConf.load(\"stable-diffusion/configs/stable-diffusion/v1-inference.yaml\")\n",
        "        sd_config.model.params.cond_stage_config.params.version = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "        # Download Stable Diffusion checkpoint if not exists\n",
        "        if not os.path.exists(\"sd-v1-4.ckpt\"):\n",
        "            print(\"Downloading Stable Diffusion checkpoint...\")\n",
        "            urllib.request.urlretrieve(\n",
        "                \"https://huggingface.co/CompVis/stable-diffusion-v-1-4-original/resolve/main/sd-v1-4.ckpt\",\n",
        "                \"sd-v1-4.ckpt\"\n",
        "            )\n",
        "\n",
        "        # Load Stable Diffusion model\n",
        "        sd_model = instantiate_from_config(sd_config.model)\n",
        "        sd_model.load_state_dict(torch.load(\"sd-v1-4.ckpt\")[\"state_dict\"], strict=False)\n",
        "        sd_model.to(self.device)\n",
        "        sd_model.eval()\n",
        "        self.sd_model = sd_model\n",
        "        self.sampler = DDIMSampler(sd_model)\n",
        "\n",
        "        # Load MiDaS model\n",
        "        print(\"Setting up MiDaS depth estimation model...\")\n",
        "        model_type = \"dpt_beit_large_512\"  # Model type\n",
        "        model_path = os.path.join(\"MiDaS\", \"weights\", \"dpt_beit_large_512.pt\")  # Path to model weights\n",
        "\n",
        "        # Load model\n",
        "        self.midas_model = load_model(model_path, model_type, self.device)\n",
        "\n",
        "        # Setup MiDaS transforms\n",
        "        self.midas_transform = torch.nn.Sequential(\n",
        "            Resize(\n",
        "                width=self.resolution,\n",
        "                height=self.resolution,\n",
        "                resize_target=False,\n",
        "                keep_aspect_ratio=True,\n",
        "                ensure_multiple_of=32,\n",
        "                resize_method=\"minimal\",\n",
        "                image_interpolation_method=cv2.INTER_CUBIC,\n",
        "            ),\n",
        "            NormalizeImage(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "            PrepareForNet(),\n",
        "        )\n",
        "\n",
        "        print(\"Models loaded successfully!\")\n",
        "\n",
        "    def generate_multiview_images(self, prompt, negative_prompt=None):\n",
        "        \"\"\"Generate multiple views of the same object from different angles.\"\"\"\n",
        "        images = []\n",
        "        angle_prompts = []\n",
        "\n",
        "        base_prompt = prompt\n",
        "        negative_prompt = negative_prompt or \"blurry, low quality, incomplete, cropped, disfigured, deformed\"\n",
        "\n",
        "        # Create angle prompts\n",
        "        view_angles = [\"front view\", \"back view\", \"side view\", \"top view\", \"bottom view\",\n",
        "                       \"45 degree angle view\", \"three-quarter view\", \"isometric view\"]\n",
        "\n",
        "        # Take only the specified number of angles\n",
        "        view_angles = view_angles[:self.angle_views]\n",
        "\n",
        "        # Generate images for each view\n",
        "        print(\"Generating multiple views...\")\n",
        "        for i, angle in enumerate(view_angles):\n",
        "            print(f\"Generating {angle}...\")\n",
        "            angle_prompt = f\"{base_prompt}, {angle}, detailed, high quality, 3D object, centered\"\n",
        "\n",
        "            # Generate image with Stable Diffusion\n",
        "            # Set up text prompt\n",
        "            c = self.sd_model.get_learned_conditioning([angle_prompt])\n",
        "            uc = self.sd_model.get_learned_conditioning([negative_prompt])\n",
        "\n",
        "            # Sample image\n",
        "            shape = [4, self.resolution // 8, self.resolution // 8]  # 4 is channels for latent space\n",
        "            samples, _ = self.sampler.sample(\n",
        "                S=self.num_inference_steps,\n",
        "                conditioning=c,\n",
        "                batch_size=1,\n",
        "                shape=shape,\n",
        "                verbose=False,\n",
        "                unconditional_guidance_scale=7.5,\n",
        "                unconditional_conditioning=uc,\n",
        "                eta=0.0\n",
        "            )\n",
        "\n",
        "            # Decode samples\n",
        "            x_samples = self.sd_model.decode_first_stage(samples)\n",
        "            x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
        "\n",
        "            # Convert to PIL image\n",
        "            x_sample = x_samples[0].cpu().numpy()\n",
        "            x_sample = (255. * x_sample.transpose(1, 2, 0)).clip(0, 255).astype(np.uint8)\n",
        "            img = Image.fromarray(x_sample)\n",
        "\n",
        "            images.append(img)\n",
        "            angle_prompts.append(angle)\n",
        "\n",
        "        return images, angle_prompts\n",
        "\n",
        "    def estimate_depth(self, image):\n",
        "        \"\"\"Estimate depth map from a single image using MiDaS.\"\"\"\n",
        "        # Convert PIL image to numpy array\n",
        "        img = np.array(image)\n",
        "\n",
        "        # Convert image to torch tensor for MiDaS\n",
        "        img_tensor = self.midas_transform({\"image\": img})[\"image\"]\n",
        "\n",
        "        # Add batch dimension\n",
        "        img_tensor = torch.unsqueeze(img_tensor, 0).to(self.device)\n",
        "\n",
        "        # Inference with MiDaS\n",
        "        with torch.no_grad():\n",
        "            # Forward pass\n",
        "            prediction = self.midas_model.forward(img_tensor)\n",
        "            # Perform disparity normalization (as done in MiDaS)\n",
        "            prediction = torch.nn.functional.interpolate(\n",
        "                prediction.unsqueeze(1),\n",
        "                size=(self.resolution, self.resolution),\n",
        "                mode=\"bicubic\",\n",
        "                align_corners=False,\n",
        "            )\n",
        "            prediction = prediction.squeeze().cpu().numpy()\n",
        "\n",
        "        # MiDaS returns inverse depth, so we need to invert it\n",
        "        depth_map = 1.0 / (prediction + 1e-6)  # Add small value to avoid division by zero\n",
        "\n",
        "        # Normalize depth map\n",
        "        depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())\n",
        "\n",
        "        # Convert to uint8 for visualization\n",
        "        depth_map = (depth_map * 255).astype(np.uint8)\n",
        "\n",
        "        # Convert to PIL Image\n",
        "        depth_image = Image.fromarray(depth_map)\n",
        "        return depth_image\n",
        "\n",
        "    def create_point_cloud_from_depth(self, image, depth_map, view_idx=0, total_views=1):\n",
        "        \"\"\"Create a point cloud from an image and its depth map.\"\"\"\n",
        "        # Convert PIL images to numpy arrays\n",
        "        img_np = np.array(image)\n",
        "        depth_np = np.array(depth_map).astype(float)\n",
        "        depth_np = depth_np / np.max(depth_np)  # Normalize depth\n",
        "\n",
        "        # Create coordinates grid\n",
        "        h, w = depth_np.shape\n",
        "        x, y = np.meshgrid(np.arange(w), np.arange(h))\n",
        "\n",
        "        # Create 3D points\n",
        "        z = depth_np.reshape(-1) * 2  # Scale depth\n",
        "        x = x.reshape(-1) - w / 2\n",
        "        y = y.reshape(-1) - h / 2\n",
        "\n",
        "        # Rotate points based on the view index\n",
        "        theta = 2 * np.pi * view_idx / total_views\n",
        "        x_rot = x * np.cos(theta) - z * np.sin(theta)\n",
        "        z_rot = x * np.sin(theta) + z * np.cos(theta)\n",
        "\n",
        "        # Create point cloud\n",
        "        points = np.vstack((x_rot, y, z_rot)).T\n",
        "\n",
        "        # Get colors from the image\n",
        "        colors = img_np.reshape(-1, 3) / 255.0\n",
        "\n",
        "        # Remove points with invalid depth\n",
        "        valid_points = ~np.isnan(points).any(axis=1)\n",
        "        points = points[valid_points]\n",
        "        colors = colors[valid_points]\n",
        "\n",
        "        return points, colors\n",
        "\n",
        "    def merge_point_clouds(self, all_points, all_colors):\n",
        "        \"\"\"Merge multiple point clouds into a single one.\"\"\"\n",
        "        merged_points = np.vstack(all_points)\n",
        "        merged_colors = np.vstack(all_colors)\n",
        "\n",
        "        # Optional: Remove duplicate or very close points\n",
        "        # This uses a voxel grid to downsample and clean the point cloud\n",
        "        pcd = o3d.geometry.PointCloud()\n",
        "        pcd.points = o3d.utility.Vector3dVector(merged_points)\n",
        "        pcd.colors = o3d.utility.Vector3dVector(merged_colors)\n",
        "\n",
        "        # Voxel downsampling to remove duplicate points\n",
        "        voxel_size = 0.01  # Adjust based on your scale\n",
        "        pcd_down = pcd.voxel_down_sample(voxel_size)\n",
        "\n",
        "        # Statistical outlier removal\n",
        "        pcd_clean, _ = pcd_down.remove_statistical_outlier(\n",
        "            nb_neighbors=20, std_ratio=2.0)\n",
        "\n",
        "        # Get final points and colors\n",
        "        final_points = np.asarray(pcd_clean.points)\n",
        "        final_colors = np.asarray(pcd_clean.colors)\n",
        "\n",
        "        return final_points, final_colors\n",
        "\n",
        "    def apply_mesh_reconstruction(self, points, colors):\n",
        "        \"\"\"Reconstruct a surface mesh from the point cloud for better quality.\"\"\"\n",
        "        # Create a point cloud object\n",
        "        pcd = o3d.geometry.PointCloud()\n",
        "        pcd.points = o3d.utility.Vector3dVector(points)\n",
        "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
        "\n",
        "        # Estimate normals for better mesh reconstruction\n",
        "        pcd.estimate_normals()\n",
        "        pcd.orient_normals_consistent_tangent_plane(100)\n",
        "\n",
        "        # Reconstruct mesh using Poisson surface reconstruction\n",
        "        print(\"Reconstructing mesh surface...\")\n",
        "        mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\n",
        "            pcd, depth=9, width=0, scale=1.1, linear_fit=False)\n",
        "\n",
        "        # Remove low density vertices\n",
        "        vertices_to_remove = densities < np.quantile(densities, 0.1)\n",
        "        mesh.remove_vertices_by_mask(vertices_to_remove)\n",
        "\n",
        "        # Convert back to point cloud for consistency\n",
        "        refined_pcd = mesh.sample_points_uniformly(number_of_points=len(points))\n",
        "\n",
        "        return np.asarray(refined_pcd.points), np.asarray(refined_pcd.colors)\n",
        "\n",
        "    def save_to_ply(self, points, colors, filename):\n",
        "        \"\"\"Save the point cloud to a PLY file.\"\"\"\n",
        "        pcd = o3d.geometry.PointCloud()\n",
        "        pcd.points = o3d.utility.Vector3dVector(points)\n",
        "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
        "\n",
        "        # Save to PLY file\n",
        "        o3d.io.write_point_cloud(filename, pcd)\n",
        "        print(f\"Point cloud saved to {filename}\")\n",
        "\n",
        "        return filename\n",
        "\n",
        "    def visualize_point_cloud(self, points, colors):\n",
        "        \"\"\"Visualize the point cloud.\"\"\"\n",
        "        pcd = o3d.geometry.PointCloud()\n",
        "        pcd.points = o3d.utility.Vector3dVector(points)\n",
        "        pcd.colors = o3d.utility.Vector3dVector(colors)\n",
        "\n",
        "        # Create a visualizer\n",
        "        vis = o3d.visualization.Visualizer()\n",
        "        vis.create_window()\n",
        "        vis.add_geometry(pcd)\n",
        "\n",
        "        # Configure visualization settings\n",
        "        opt = vis.get_render_option()\n",
        "        opt.point_size = 1.0\n",
        "        opt.background_color = np.array([0.1, 0.1, 0.1])\n",
        "\n",
        "        # Run the visualizer\n",
        "        vis.run()\n",
        "        vis.destroy_window()\n",
        "\n",
        "    def process(self, prompt, negative_prompt=None, output_filename=\"output.ply\", visualize=True):\n",
        "        \"\"\"Process a text prompt to generate a 3D point cloud.\"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        print(f\"Processing prompt: '{prompt}'\")\n",
        "\n",
        "        # Generate multi-view images\n",
        "        images, angle_prompts = self.generate_multiview_images(prompt, negative_prompt)\n",
        "\n",
        "        # Display generated images\n",
        "        fig, axes = plt.subplots(1, len(images), figsize=(15, 3))\n",
        "        for i, (img, angle) in enumerate(zip(images, angle_prompts)):\n",
        "            if len(images) == 1:\n",
        "                axes.imshow(img)\n",
        "                axes.set_title(angle)\n",
        "                axes.axis('off')\n",
        "            else:\n",
        "                axes[i].imshow(img)\n",
        "                axes[i].set_title(angle)\n",
        "                axes[i].axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Process each image to get point clouds\n",
        "        all_points = []\n",
        "        all_colors = []\n",
        "\n",
        "        print(\"Converting images to point clouds...\")\n",
        "        for i, image in enumerate(images):\n",
        "            depth_map = self.estimate_depth(image)\n",
        "\n",
        "            # Display depth map\n",
        "            plt.figure(figsize=(5, 5))\n",
        "            plt.imshow(depth_map, cmap='plasma')\n",
        "            plt.title(f\"Depth Map - {angle_prompts[i]}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            # Create point cloud from this view\n",
        "            points, colors = self.create_point_cloud_from_depth(\n",
        "                image, depth_map, view_idx=i, total_views=len(images))\n",
        "\n",
        "            all_points.append(points)\n",
        "            all_colors.append(colors)\n",
        "\n",
        "        # Merge all point clouds\n",
        "        print(\"Merging point clouds from all views...\")\n",
        "        merged_points, merged_colors = self.merge_point_clouds(all_points, all_colors)\n",
        "\n",
        "        # Apply mesh reconstruction for better quality\n",
        "        refined_points, refined_colors = self.apply_mesh_reconstruction(merged_points, merged_colors)\n",
        "\n",
        "        # Save point cloud\n",
        "        output_path = self.save_to_ply(refined_points, refined_colors, output_filename)\n",
        "\n",
        "        # Optionally visualize\n",
        "        if visualize:\n",
        "            print(\"Visualizing point cloud...\")\n",
        "            self.visualize_point_cloud(refined_points, refined_colors)\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Processing completed in {elapsed_time:.2f} seconds\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "# Example usage in Colab\n",
        "def main():\n",
        "    # Initialize the model\n",
        "    generator = TextTo3DPointCloud()\n",
        "\n",
        "    # Get user input for text description\n",
        "    text_prompt = input(\"Enter a description of the 3D object you want to generate: \")\n",
        "    output_filename = input(\"Enter output filename (default: output.ply): \") or \"output.ply\"\n",
        "\n",
        "    # Generate point cloud\n",
        "    output_path = generator.process(\n",
        "        prompt=text_prompt,\n",
        "        negative_prompt=\"blurry, low quality, incomplete, cropped, disfigured, deformed\",\n",
        "        output_filename=output_filename,\n",
        "        visualize=True\n",
        "    )\n",
        "\n",
        "    print(f\"3D point cloud saved to: {output_path}\")\n",
        "\n",
        "    # Provide code for downloading the file\n",
        "    from google.colab import files\n",
        "    files.download(output_path)\n",
        "\n",
        "    # Show visualization of the 3D point cloud using Open3D's web visualization\n",
        "    print(\"\\nDisplaying 3D point cloud in a web viewer...\")\n",
        "    pcd = o3d.io.read_point_cloud(output_path)\n",
        "    o3d.visualization.draw_geometries([pcd])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Print banner\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Text to 3D Point Cloud Generator with MiDaS Depth Estimation\")\n",
        "    print(\"=\" * 80)\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "Ptf3i1v4-Uc-",
        "outputId": "4e74d96d-34b2-44a0-e490-9560731df812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch3d (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch3d\u001b[0m\u001b[31m\n",
            "\u001b[0mDownloading MiDaS model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ldm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-320a9f401bbd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Import Stable Diffusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mddim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDDIMSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minstantiate_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0momegaconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ldm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}